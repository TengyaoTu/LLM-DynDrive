# -*- coding: utf-8 -*-
"""
hidden_analysis_mixed_auto.py

åœ¨åŸç‰ˆåŸºç¡€ä¸Šï¼Œä¿ç•™ï¼š
  - S = mean(H_hit) - mean(H_non) çš„ä¿å­˜
  - â€œÎ¼_hit â†’ H_nonâ€ä¸â€œÎ¼_non â†’ H_hitâ€çš„æœ€çŸ­/æœ€é•¿æ¬§æ°è·ç¦»ç»Ÿè®¡

æ–°å¢ï¼ˆå‘½ä»¤è¡Œä¸å˜å³å¯è¿è¡Œï¼›æ–°å¢å‚æ•°å‡æœ‰é»˜è®¤å€¼ï¼‰ï¼š
  - å¯¹è§’è¿‘ä¼¼ LDA åˆ†å‰²ï¼Œè®¡ç®—ï¼š
      * æ²¿ -u_w çš„æœ€çŸ­è¦†ç›–è·ç¦» d_all_w
      * å›ºå®šæ²¿ -S çš„æœ€çŸ­è¦†ç›–è·ç¦» d_all_S ä¸ç³»æ•° alpha_all_S
      * ä»…å‡å€¼è¶Šç•Œçš„ d_mean_w, d_mean_S, alpha_mean_S
  - æå– jsonl ä¸­æ‰€æœ‰ä¿¡å¿ƒå€¼å¹¶è®¡ç®—åˆ†ä½æ•° q25 / q75
  - å‘½ä»¤è¡Œå…¼å®¹ä¿ç•™ --gamma/--epsilon/--deltaï¼Œä½†ä¸å†ç”¨äºâ€œå®‰å…¨ä½™é‡â€è¿ç®—
"""

import os
import json
import argparse
from typing import Tuple

import torch
from torch.utils.data import ConcatDataset

# ===== ä½ çš„å·¥ç¨‹å†…å·²æœ‰çš„æ„å»ºæ•°æ®é›†å‡½æ•° =====
try:
    from hidden_analysis_mixed import batch_build_all_mixed
except Exception as e:
    raise ImportError(
        "æ— æ³•ä» hidden_analysis_mixed å¯¼å…¥ batch_build_all_mixedã€‚è¯·ç¡®è®¤è¯¥æ–‡ä»¶ä¸æœ¬è„šæœ¬åŒç›®å½•æˆ–åœ¨ PYTHONPATH ä¸­ã€‚"
        f" åŸå§‹é”™è¯¯ï¼š{repr(e)}"
    )


def compute_confidence_quartiles(jsonl_path: str):
    # ä» JSONL ä¸­å°½å¯èƒ½é²æ£’åœ°æŠ½å–æ‰€æœ‰â€œä¿¡å¿ƒå€¼â€ï¼Œè¿”å› (q25, q75, N)ã€‚
    # æ”¯æŒé”®ï¼š
    #   - ç›´æ¥é”®ï¼š "confidence", "conf", "score"
    #   - åˆ—è¡¨é”®ï¼š "sentence_confidences", "confidences", "scores", "sentence_scores", "step_confidences"
    #   - ä»»æ„å±‚çº§åµŒå¥—æ—¶ï¼Œè‹¥å…ƒç´ æ˜¯ dict å¹¶å«æœ‰ä¸Šè¿°ç›´æ¥é”®ï¼Œä¹Ÿä¼šè¢«æŠ½å–
    import json, math
    import torch

    def _collect_from_obj(o, sink):
        if isinstance(o, dict):
            for k, v in o.items():
                lk = k.lower()
                # ç›´æ¥æ•°å€¼
                if lk in ("confidence", "conf", "score") and isinstance(v, (int, float)) and math.isfinite(v):
                    sink.append(float(v))
                # åˆ—è¡¨å­—æ®µ
                elif lk in ("sentence_confidences", "confidences", "scores", "sentence_scores", "step_confidences"):
                    if isinstance(v, (list, tuple)):
                        for x in v:
                            if isinstance(x, (int, float)) and math.isfinite(x):
                                sink.append(float(x))
                            elif isinstance(x, dict):
                                for kk in ("confidence", "conf", "score"):
                                    if kk in x and isinstance(x[kk], (int, float)) and math.isfinite(x[kk]):
                                        sink.append(float(x[kk]))
                # é€’å½’éå†å…¶ä»–åµŒå¥—
                if isinstance(v, (dict, list, tuple)):
                    _collect_from_obj(v, sink)
        elif isinstance(o, (list, tuple)):
            for x in o:
                _collect_from_obj(x, sink)

    confidences = []
    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue
                _collect_from_obj(obj, confidences)
    except FileNotFoundError:
        return None, None, 0

    if not confidences:
        return None, None, 0

    t = torch.tensor(confidences, dtype=torch.float64)
    q25 = torch.quantile(t, 0.25).item()
    q75 = torch.quantile(t, 0.75).item()
    return q25, q75, len(confidences)


# ====== å·¥å…·ï¼šæ”¶é›†ä¸¤ç±»å‘é‡ ======
def _collect_vectors_by_class(merged_dataset: ConcatDataset) -> Tuple[torch.Tensor, torch.Tensor]:
    """ä»åˆå¹¶æ•°æ®é›†æ”¶é›†ä¸¤ç±»å‘é‡ï¼Œè¿”å› H_hit, H_nonï¼ˆå‡ä¸º [N, D] çš„ float32 Tensorï¼‰"""
    hits, nonhits = [], []
    for i in range(len(merged_dataset)):
        feat, y = merged_dataset[i]
        feat = feat.view(-1).to(torch.float32)  # ä¿è¯ 1D
        label = int(y.item()) if hasattr(y, "item") else int(y)
        if label == 1:
            hits.append(feat)
        else:
            nonhits.append(feat)
    if len(hits) == 0 or len(nonhits) == 0:
        raise ValueError(f"ç±»åˆ«ä¸å¹³è¡¡ï¼šhit={len(hits)}, non={len(nonhits)}")
    H_hit = torch.stack(hits, dim=0)   # [N1, D]
    H_non = torch.stack(nonhits, dim=0)  # [N0, D]
    return H_hit, H_non


# ====== å·¥å…·ï¼šä¸­å¿ƒâ†’é›†åˆæœ€çŸ­/æœ€é•¿è·ç¦»ï¼ˆåˆ†å—ï¼‰ ======
@torch.no_grad()
def center_to_set_minmax(center: torch.Tensor,
                         H: torch.Tensor,
                         block_size: int = 65536,
                         device: str = "cpu") -> Tuple[float, float]:
    """
    è®¡ç®—â€œä¸­å¿ƒå‘é‡ center åˆ°é›†åˆ H çš„æœ€çŸ­/æœ€é•¿æ¬§æ°è·ç¦»â€ï¼Œåˆ†å—ä»¥èŠ‚çœæ˜¾å­˜/å†…å­˜ã€‚
    center: [D], H: [N, D]
    è¿”å›: (min_dist, max_dist)
    """
    assert center.dim() == 1 and H.dim() == 2 and center.size(0) == H.size(1), \
        f"ç»´åº¦ä¸åŒ¹é…: center {center.shape}, H {H.shape}"
    center = center.to(torch.float32).to(device)
    H = H.to(torch.float32).to(device)

    n = H.size(0)
    min_d = float("inf")
    max_d = 0.0
    for s in range(0, n, block_size):
        e = min(s + block_size, n)
        block = H[s:e]  # [B, D]
        diff = block - center  # [B, D]
        d = torch.linalg.norm(diff, dim=1)  # [B]
        min_d = min(min_d, float(d.min().item()))
        max_d = max(max_d, float(d.max().item()))
    return min_d, max_d


# ====== æ ¸å¿ƒï¼šæ„å»ºä¸­å¿ƒä¸èŒƒå›´ ======
@torch.no_grad()
def build_centers_and_ranges(merged_dataset: ConcatDataset,
                             center_block_size: int = 65536,
                             device: str = "cpu"):
    """
    ä»åˆå¹¶æ•°æ®é›†ä¸­æ„é€ ï¼š
      - S = mean(H_hit) - mean(H_non)
      - H_hit ä¸ H_non ä¸¤ä¸ªç±»çš„ä¸­å¿ƒ Î¼_hit, Î¼_non
      - Î¼_hit â†’ H_non çš„æœ€çŸ­/æœ€é•¿è·ç¦»ï¼›Î¼_non â†’ H_hit çš„æœ€çŸ­/æœ€é•¿è·ç¦»
    """
    H_hit, H_non = _collect_vectors_by_class(merged_dataset)  # [N1, D], [N0, D]

    # è®¡ç®—ä¸­å¿ƒ
    mu_hit = H_hit.mean(dim=0)  # [D]
    mu_non = H_non.mean(dim=0)  # [D]
    S = (mu_hit - mu_non).to(torch.float32)  # [D]

    # è·ç¦»èŒƒå›´ï¼ˆåˆ†å—ï¼‰
    hit_to_non_min, hit_to_non_max = center_to_set_minmax(mu_hit, H_non, center_block_size, device)
    non_to_hit_min, non_to_hit_max = center_to_set_minmax(mu_non, H_hit, center_block_size, device)

    return S, mu_hit, mu_non, H_hit, H_non, hit_to_non_min, hit_to_non_max, non_to_hit_min, non_to_hit_max


# ====== LDAï¼ˆå¯¹è§’è¿‘ä¼¼ï¼‰ä¸ steer è®¡ç®— ======
@torch.no_grad()
def compute_lda_separator_and_steer(merged_dataset: ConcatDataset, ridge: float = 0.0):
    """
    ä½¿ç”¨â€œå¯¹è§’åæ–¹å·®è¿‘ä¼¼â€çš„ LDAï¼Œå¾—åˆ°åˆ†å‰²å‘é‡ w ä¸é˜ˆå€¼ bï¼Œä½¿å¾— sign(wÂ·x + b) ä½œä¸ºçº¿æ€§åˆ¤åˆ«ã€‚
    ä»…ç”¨äºåˆ†æ/æ–¹å‘ä¼°è®¡ï¼Œä¸åšåˆ†ç±»è®­ç»ƒã€‚
    """
    H_hit, H_non = _collect_vectors_by_class(merged_dataset)
    X1 = H_hit.to(torch.float64)
    X0 = H_non.to(torch.float64)

    mu1 = X1.mean(dim=0)
    mu0 = X0.mean(dim=0)
    S1 = X1.var(dim=0, unbiased=False)  # å¯¹è§’è¿‘ä¼¼
    S0 = X0.var(dim=0, unbiased=False)

    # åŠ å¾®å° ridgeï¼Œé¿å…æå°æ–¹å·®å¯¼è‡´æ•°å€¼ä¸ç¨³
    if ridge > 0:
        S1 = S1 + ridge
        S0 = S0 + ridge

    # å¯¹è§’ LDAï¼šw = (mu1 - mu0) / (S1 + S0)
    denom = S1 + S0 + 1e-12
    w = (mu1 - mu0) / denom
    w = w.to(torch.float64)

    # åˆ¤åˆ«é˜ˆå€¼ bï¼Œé‡‡ç”¨ Fisher åˆ¤åˆ«çš„å‡å€¼ä¸­ç‚¹ï¼ˆç®€åŒ–å¤„ç†ï¼‰
    # ç›®æ ‡ï¼šwÂ·x + b > 0 å½’ä¸º 1 ç±»
    m1 = (w * mu1).sum().item()
    m0 = (w * mu0).sum().item()
    t = -0.5 * (m1 + m0)  # é˜ˆå€¼ t = -b
    b = -t

    # ä¸€äº›æ–¹å‘èŒƒæ•°
    S_vec = (mu1 - mu0)
    w_norm = float(torch.linalg.norm(w).item())
    S_norm = float(torch.linalg.norm(S_vec).item())
    if w_norm < 1e-12 or S_norm < 1e-12:
        return {"ok": False, "reason": "æ–¹å‘èŒƒæ•°è¿‡å°"}

    # æ²¿ -u_w çš„â€œå…¨ä½“è¶Šç•Œâ€æœ€çŸ­è·ç¦»ï¼ˆä½¿æ‰€æœ‰ hit æ ·æœ¬è¶Šè¿‡åˆ¤åˆ«é¢ï¼‰
    # å¯¹æ¯ä¸ª hit æ ·æœ¬ hï¼šéœ€è¦ d_i ä½¿ t - wÂ·h + d_i * ||w|| >= 0
    # => d_i >= (wÂ·h - t) / ||w||
    proj_hit = (X1 @ w).double()  # [N1]
    req = (proj_hit - t) / (w_norm + 1e-12)  # [N1]
    d_all_w = float(req.max().item())

    # å›ºå®šæ²¿ -S æ–¹å‘ï¼ˆå•ä½å‘é‡ u_Sï¼‰
    uS = S_vec / (S_norm + 1e-12)
    w_dot_uS = float((w * uS).sum().item())  # wÂ·uS
    if w_dot_uS <= 0:
        # è‹¥ w ä¸ -S æ–¹å‘å¤¹è§’è¿‡å¤§ï¼Œæ²¿ -S æ— æ³•é™ä½åˆ¤åˆ«å€¼
        d_all_S = float("inf")
        alpha_all_S = float("inf")
        d_mean_S = float("inf")
        alpha_mean_S = float("inf")
        d_mean_w = max((m1 - t) / (w_norm + 1e-12), 0.0)
    else:
        # ä½¿æ‰€æœ‰ hit è¶Šç•Œæ‰€éœ€æŠ•å½±è·ç¦»ï¼ˆæ²¿ -uS çš„æ¬§æ°è·ç¦»ï¼‰ï¼šmax_i (wÂ·h_i - t) / (wÂ·uS)
        alpha_u_all = float(((proj_hit - t) / (w_dot_uS + 1e-12)).max().item())
        # å°†â€œæ²¿ -uS çš„è·ç¦»â€æ¢ç®—ä¸ºâ€œæ²¿ -Sï¼ˆéå•ä½å‘é‡ï¼‰çš„ç³»æ•°â€ï¼šalpha_S = alpha_u / ||S||
        alpha_all_S = alpha_u_all / (S_norm + 1e-12)
        # Euclidean è·ç¦»æ²¿ -uS çš„æœ€çŸ­ç§»åŠ¨é‡
        d_all_S = alpha_u_all

        # ä»…å‡å€¼è¶Šç•Œï¼šmu1
        d_mean_w = max((m1 - t) / (w_norm + 1e-12), 0.0)
        d_mean_S_u = max((m1 - t) / (w_dot_uS + 1e-12), 0.0)   # æ²¿ -uS çš„æ¬§æ°è·ç¦»
        alpha_mean_S = d_mean_S_u / (S_norm + 1e-12)           # æ¢ç®—ä¸ºæ²¿ -S çš„ç³»æ•°
        d_mean_S = d_mean_S_u

    return {
        "ok": True,
        "w_norm": w_norm,
        "S_norm": S_norm,
        "w": w,
        "S": S_vec.to(torch.float64),
        "b": b,
        "threshold_t": t,
        "max_w_dot_h": float(proj_hit.max().item()),
        "d_all_w": d_all_w,
        "d_all_S": d_all_S,
        "alpha_all_S": alpha_all_S,
        "d_mean_w": float((m1 - t) / (w_norm + 1e-12) if w_norm > 0 else float("inf")),
        "d_mean_S": d_mean_S,
        "alpha_mean_S": alpha_mean_S,
        "w_dot_uS": w_dot_uS,
    }


def main():
    parser = argparse.ArgumentParser(
        description="Compute S, center-to-other distance ranges, and LDA-based steer thresholds (command unchanged)"
    )
    parser.add_argument("--layer_id", type=int, required=True)
    parser.add_argument("--jsonl_path", type=str, required=True)
    parser.add_argument("--hidden_dir", type=str, required=True)
    parser.add_argument("--save_path", type=str, required=True,
                        help="ä¿å­˜å‡å€¼å·®å‘é‡ S çš„ .pt è·¯å¾„")
    parser.add_argument("--threshold", type=float, default=0.7)
    parser.add_argument("--max_files", type=int, default=100)
    parser.add_argument("--expected_offset", type=int, default=1)
    parser.add_argument("--verbose", action="store_true")

    # å…¼å®¹æ—§å‚æ•°ï¼ˆå¿½ç•¥ä¸ç”¨ï¼‰
    parser.add_argument("--pairwise_block_rows", type=int, default=2048, help="[å…¼å®¹å‚æ•°] å·²å¿½ç•¥")
    parser.add_argument("--pairwise_block_cols", type=int, default=4096, help="[å…¼å®¹å‚æ•°] å·²å¿½ç•¥")

    # æœ‰æ•ˆå‚æ•°ï¼ˆä¸åŸè„šæœ¬ä¸€è‡´ï¼‰
    parser.add_argument("--center_block_size", type=int, default=65536,
                        help="ä¸­å¿ƒåˆ°é›†åˆè·ç¦»çš„åˆ†å—å¤§å°ï¼ˆä¸€æ¬¡å¤„ç†çš„å‘é‡æ¡æ•°ï¼‰")
    parser.add_argument("--device", type=str, choices=["cpu", "cuda"], default="cpu")
    parser.add_argument("--report_path", type=str, default="",
                        help="å¯é€‰ï¼šå†™å‡º JSON æŠ¥å‘Šåˆ°è¯¥è·¯å¾„")

    # ä¸ºä¿æŒå‘½ä»¤è¡Œå…¼å®¹ï¼Œä»¥ä¸‹ä¸‰ä¸ªå‚æ•°ä»ä¿ç•™ï¼Œä½†ä¸å†å‚ä¸ä»»ä½•â€œå®‰å…¨ä½™é‡â€è®¡ç®—
    parser.add_argument("--gamma", type=float, default=1.5, help="[å…¼å®¹ä¿ç•™]")
    parser.add_argument("--epsilon", type=float, default=150, help="[å…¼å®¹ä¿ç•™]")
    parser.add_argument("--delta", type=float, default=35, help="[å…¼å®¹ä¿ç•™]")

    args = parser.parse_args()

    # æ„å»º (feat, label) åˆå¹¶æ•°æ®é›†
    merged = batch_build_all_mixed(
        layer_id=args.layer_id,
        jsonl_path=args.jsonl_path,
        hidden_dir=args.hidden_dir,
        threshold=args.threshold,
        max_files=args.max_files,
        expected_offset=args.expected_offset,
        verbose=args.verbose
    )

    (S, mu_hit, mu_non,
     H_hit, H_non,
     hit_to_non_min, hit_to_non_max,
     non_to_hit_min, non_to_hit_max) = build_centers_and_ranges(
        merged,
        center_block_size=args.center_block_size,
        device=args.device
    )

    # ä¿å­˜ S
    if os.path.dirname(args.save_path):
        os.makedirs(os.path.dirname(args.save_path), exist_ok=True)
    torch.save(S.cpu(), args.save_path)

    # æ‰“å°ä¸­å¿ƒâ†’é›†åˆè·ç¦»ä¸ S
    dim = S.numel()
    l2 = float(torch.linalg.norm(S).item())
    print(f"âœ… å·²ä¿å­˜ steer å‘é‡åˆ° {args.save_path}")
    print(f"   dim={dim} | ||S||_2 = {l2:.6f} | hit={H_hit.size(0)} | nonhit={H_non.size(0)}")
    print(f"   Î¼_hit â†’ H_non  æœ€çŸ­è·ç¦»: {hit_to_non_min:.6f} | æœ€é•¿è·ç¦»: {hit_to_non_max:.6f}")
    print(f"   Î¼_non â†’ H_hit  æœ€çŸ­è·ç¦»: {non_to_hit_min:.6f} | æœ€é•¿è·ç¦»: {non_to_hit_max:.6f}")

    # ===== LDA åˆ†å‰²ä¸ steerï¼ˆridge=0ï¼Œæ— å…ˆéªŒï¼›ä¿ç•™åŸæœ‰æ ¸å¿ƒæŒ‡æ ‡ï¼‰ =====
    lda = compute_lda_separator_and_steer(merged)
    if lda.get("ok", False):
        print("\nâ€”â€” LDA(å¯¹è§’) åˆ†å‰²ä¸ steer â€”â€”")
        print(f"  â€¢ ||w||_2 = {lda['w_norm']:.6f} | ||S||_2 = {lda['S_norm']:.6f}")
        print(f"  â€¢ åˆ¤åˆ«é˜ˆå€¼ t = -b = {lda['threshold_t']:.6f} | max_i(wÂ·h_i) = {lda['max_w_dot_h']:.6f}")

        print("  â€¢ æœ€ä¼˜æ–¹å‘ï¼ˆæ²¿ -u_wï¼‰:")
        print(f"      d_all_w  = {lda['d_all_w']:.6f}   # ä½¿æ‰€æœ‰ hit è¿›å…¥ non åŒºåŸŸçš„æœ€çŸ­æ¬§æ°è·ç¦»")

        print("  â€¢ å›ºå®šä½ çš„æ–¹å‘ï¼ˆæ²¿ -Sï¼‰:")
        if lda['d_all_S'] == float('inf'):
            print("      ä¸ w æ–¹å‘å¤¹è§’è¿‡å¤§ï¼Œæ²¿ -S æ— æ³•é™ä½åˆ¤åˆ«å€¼ï¼›è¯·æ”¹ç”¨ -w æˆ–æ£€æŸ¥ S çš„å®šä¹‰ã€‚")
        else:
            print(f"      d_all_S  = {lda['d_all_S']:.6f}   | alpha_all_S = {lda['alpha_all_S']:.6f}")

        print("  â€¢ ä»…å‡å€¼è¶Šç•Œï¼š")
        if lda['d_mean_S'] == float('inf'):
            print(f"      d_mean_w = {lda['d_mean_w']:.6f} | d_mean_S æ— æ³•å®šä¹‰ï¼ˆwÂ·S â‰¤ 0ï¼‰")
        else:
            print(f"      d_mean_w = {lda['d_mean_w']:.6f} | d_mean_S = {lda['d_mean_S']:.6f} | alpha_mean_S = {lda['alpha_mean_S']:.6f}")
    else:
        print(f"âš ï¸ LDA åˆ†å‰²å¤±è´¥ï¼š{lda.get('reason','unknown')}")

    # ===== ç»Ÿè®¡æ‰€æœ‰â€œä¿¡å¿ƒå€¼â€çš„ q25 / q75 å¹¶æ‰“å° =====
    q25, q75, n_conf = compute_confidence_quartiles(args.jsonl_path)
    if n_conf > 0:
        print(f"   confidence åˆ†ä½æ•°: q25 = {q25:.6f} | q75 = {q75:.6f} | N = {n_conf}")
    else:
        print("   æœªä» JSONL æå–åˆ°ä»»ä½• confidence å€¼ï¼›è·³è¿‡åˆ†ä½æ•°ç»Ÿè®¡ã€‚")

    # ===== å¯é€‰æŠ¥å‘Šè½ç›˜ =====
    if args.report_path:
        report = {
            "layer_id": args.layer_id,
            "dim": dim,
            "n_hit": H_hit.size(0),
            "n_non": H_non.size(0),
            "S_l2": l2,
            "hit_to_non_min": hit_to_non_min,
            "hit_to_non_max": hit_to_non_max,
            "non_to_hit_min": non_to_hit_min,
            "non_to_hit_max": non_to_hit_max,
            "jsonl_path": args.jsonl_path,
            "hidden_dir": args.hidden_dir,
            "threshold": args.threshold,
            "center_block_size": args.center_block_size,
            "device": args.device,
            "confidence_q25": q25,
            "confidence_q75": q75,
            "confidence_N": n_conf,
        }
        if lda.get("ok", False):
            report.update({
                "w_norm": lda["w_norm"],
                "S_norm": lda["S_norm"],
                "b": lda["b"],
                "threshold_t": lda["threshold_t"],
                "max_w_dot_h": lda["max_w_dot_h"],
                "margin_needed": lda["d_all_w"],
                "d_all_w": lda["d_all_w"],
                "d_all_S": lda["d_all_S"],
                "alpha_all_S": lda["alpha_all_S"],
                "d_mean_w": lda["d_mean_w"],
                "d_mean_S": lda["d_mean_S"],
                "alpha_mean_S": lda["alpha_mean_S"],
                "w_dot_uS": lda["w_dot_uS"],
            })
        if os.path.dirname(args.report_path):
            os.makedirs(os.path.dirname(args.report_path), exist_ok=True)
        with open(args.report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“ å·²å†™å‡ºæŠ¥å‘Š: {args.report_path}")


if __name__ == "__main__":
    main()
