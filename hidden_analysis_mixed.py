# -*- coding: utf-8 -*-
import os
import re
import json
import argparse
from typing import Optional

import torch
from torch.utils.data import TensorDataset, ConcatDataset

# =========================
# è¯è¡¨ï¼ˆå‘½ä¸­å³è§†ä¸ºâ€œåŒ…å«è¯æ±‡â€ï¼‰
# æ¥è‡ªå…³é”®è¯è„šæœ¬ï¼Œä¿ç•™åŸæœ‰æ¨¡å¼ä¸æ­£åˆ™æ„é€ 
# =========================
LEXICON_BASE = [
    # ä¸ç¡®å®š/åæ€ç±»çš„ä½ä¿¡å¿ƒè¯æ±‡////è®¡ç®—ç±»/ä¸ç†Ÿæ‚‰ç±»çš„ä½ä¿¡å¿ƒè¯æ±‡
    "alternatively", "alternative", "another", "perhaps", "maybe", "wait", "but",
    "think again", "make sure", "just to ensure", "there any other", "some other",
    "should consider", "about whether", "if they have", "i was", "any errors",
    "or something", "let me check", "hold on", "double check", "however",
    "confusing", "differently", "careful", "sometimes", "alternate"
]

def _normalize_text(s: str) -> str:
    # å¼•å· & ç ´æŠ˜å·/è¿å­—ç¬¦ç»Ÿä¸€
    return (s or "")\
        .replace("â€™", "'").replace("â€˜", "'").replace("â€œ", '"').replace("â€", '"')\
        .replace("â€“", "-").replace("â€”", "-").replace("-", "-")  # en/em/nb-hyphen

def _token_pattern(tok: str) -> str:
    """
    æŠŠä¸€ä¸ªè¯/çŸ­è¯­è½¬æˆæ›´é²æ£’çš„ regex ç‰‡æ®µï¼š
    - ç©ºç™½æŠ˜å ä¸º \s+
    - å…è®¸ 'double[-\s]check' è¿™ç§ç”¨ç©ºæ ¼æˆ–è¿å­—ç¬¦è¿æ¥
    - ç»™å¸¸è§è¯å°¾åŠ å¯é€‰å˜ä½“
    """
    tok = _normalize_text(tok).lower().strip()
    parts = re.split(r"\s+", tok)

    def word2regex(w: str) -> str:
        if w == "verify":
            return r"verif(?:y|ies|ied|ying|ication(?:s)?)"
        if w == "alternative":
            return r"alternative(?:s|ly)?"
        if w == "confusing":
            return r"confus(?:e|es|ed|ing)"
        if w == "differently":
            return r"different(?:ly)?"
        if w == "careful":
            return r"careful(?:ly)?"
        if w == "sometimes":
            return r"sometimes?"
        if w == "alternate":
            return r"alternat(?:e|es|ed|ing)"
        # é»˜è®¤ä¸¥æ ¼è¯å½¢
        return re.escape(w)

    if len(parts) >= 2:
        pieces = [word2regex(p) for p in parts]
        between = r"(?:[-\s]+)"  # ç©ºæ ¼æˆ–è¿å­—ç¬¦
        body = between.join(pieces)
    else:
        body = word2regex(parts[0])

    return rf"\b{body}\b"

def _compile_lexicon_regex(lexicon_base):
    expanded = []
    for term in lexicon_base:
        t = term.strip()
        if not t:
            continue
        expanded.append(_token_pattern(t))
    pattern = "|".join(expanded)
    return re.compile(pattern, flags=re.IGNORECASE)

LEXICON_RE = _compile_lexicon_regex(LEXICON_BASE)

def has_lexicon_hit(text: str) -> int:
    return 1 if LEXICON_RE.search(_normalize_text(text).lower()) else 0


# ----------------------
# Utilities
# ----------------------
def read_jsonl(path: str):
    with open(path, 'r', encoding='utf-8') as f:
        return [json.loads(line.strip()) for line in f if line.strip()]

def split_segments_for_conf(resp_text: str):
    """
    ä»…ç»Ÿè®¡ </think> ä¹‹å‰çš„å†…å®¹ï¼›å†æŒ‰è¿ç»­ç©ºè¡Œåˆ†æ®µã€‚
    """
    text = resp_text or ""
    if "</think>" in text:
        text = text.split("</think>")[0]
    segs = re.split(r"\n\n+", text)
    segs = [s.strip() for s in segs if len(s.strip()) > 0]
    return segs


# ----------------------
# Dataset builder: (feature, label_mixed)
# label_mixed = 1 å½“ (å…³é”®è¯å‘½ä¸­) OR (conf < threshold)ï¼Œå¦åˆ™ 0
# å¯¹é½ç­–ç•¥ä¸ä¸¤ä¸ªæ¥æºè„šæœ¬ä¸€è‡´ï¼Œåšæœ€å°åˆå¹¶ï¼š
#   - V = hidden['step'].shape[0]
#   - C_raw = len(sentence_confidences)
#   - æœŸæœ› V == C_raw - expected_offset
#   - å…³é”®è¯åˆ†æ®µæ¥è‡ª generated_responses çš„ </think> å‰å†…å®¹
#   - æœ€ç»ˆé•¿åº¦ m = min(V, len(lex_labels), len(confs_aligned))
# ----------------------
def build_dataset_from_layer_mixed(
    layer_id: int,
    json_item: dict,
    hidden_path: str,
    threshold: float = 0.7,
    expected_offset: int = 1,
    verbose: bool = False
) -> Optional[TensorDataset]:
    confs_raw = json_item.get("sentence_confidences", [])
    if not confs_raw or len(confs_raw) <= expected_offset:
        if verbose:
            print("â›” è·³è¿‡ï¼šsentence_confidences ä¸ºç©ºæˆ–è¿‡çŸ­")
        return None

    if not os.path.exists(hidden_path):
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {hidden_path}")
        return None

    try:
        data = torch.load(hidden_path, map_location='cpu', weights_only=False)
    except Exception as e:
        if verbose:
            print(f"âŒ åŠ è½½å¤±è´¥ {hidden_path}: {e}")
        return None

    if layer_id not in data:
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šlayer {layer_id} ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­")
        return None

    layer_data = data[layer_id]
    sample_id = 0
    if sample_id not in layer_data or 'step' not in layer_data[sample_id]:
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šç¼ºå¤± sample_id={sample_id} æˆ– step")
        return None

    tensor = layer_data[sample_id]['step']  # [V, hidden_dim]
    V = tensor.shape[0]
    C_raw = len(confs_raw)

    if V != C_raw - expected_offset:
        if verbose:
            print(f"âŒ å‘é‡æ•° {V} ä¸åŸå§‹ä¿¡å¿ƒæ•° {C_raw} ä¸æ»¡è¶³å·® {expected_offset} -> è·³è¿‡")
        return None

    # 1) å…³é”®è¯æ ‡ç­¾ï¼ˆä¸å…³é”®è¯è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    responses_text = (json_item.get("generated_responses") or [""])[0]
    segs = split_segments_for_conf(responses_text)
    if expected_offset > 0 and len(segs) >= expected_offset:
        segs = segs[expected_offset:]
    lex_labels = [float(has_lexicon_hit(s)) for s in segs]  # 0/1

    # 2) ç½®ä¿¡åº¦æ ‡ç­¾ï¼ˆä¸ç½®ä¿¡åº¦è„šæœ¬ä¿æŒä¸€è‡´ï¼šä½ä¿¡å¿ƒ=1ï¼Œé«˜ä¿¡å¿ƒ=0ï¼‰
    confs = confs_raw[expected_offset:]
    # æˆªæ–­ä¿è¯ä¸ä¼šè¶…å‡º V
    confs = confs[:V]
    conf_labels = [1.0 if float(c) < threshold else 0.0 for c in confs]

    # 3) å¯¹é½é•¿åº¦ m
    m = min(V, len(lex_labels), len(conf_labels))
    if m <= 0:
        if verbose:
            print("âŒ å¯¹é½åé•¿åº¦ä¸º 0 -> è·³è¿‡")
        return None

    features = tensor[:m].to(dtype=torch.float32)
    lex_labels_t = torch.tensor(lex_labels[:m], dtype=torch.float32)
    conf_labels_t = torch.tensor(conf_labels[:m], dtype=torch.float32)

    # 4) Mixed è§„åˆ™ï¼šåªè¦å‘½ä¸­å…³é”®è¯ OR ä½ä¿¡å¿ƒå³ä¸º 1
    labels_mixed = torch.clamp(lex_labels_t + conf_labels_t, max=1.0)

    return TensorDataset(features, labels_mixed)


def batch_build_all_mixed(
    layer_id: int,
    jsonl_path: str,
    hidden_dir: str,
    threshold: float = 0.7,
    max_files: int = 100,
    expected_offset: int = 1,
    verbose: bool = True
) -> ConcatDataset:
    data_json = read_jsonl(jsonl_path)
    datasets = []
    total = min(max_files, len(data_json))
    kept = 0
    for i in range(total):
        hidden_path = os.path.join(hidden_dir, f"hidden_{i}.pt")
        ds = build_dataset_from_layer_mixed(
            layer_id=layer_id,
            json_item=data_json[i],
            hidden_path=hidden_path,
            threshold=threshold,
            expected_offset=expected_offset,
            verbose=False
        )
        if ds is not None:
            datasets.append(ds)
            kept += len(ds)
        else:
            if verbose:
                print(f"[skip] index={i}")
        if (i + 1) % 50 == 0 and verbose:
            print(f"[progress] processed {i+1}/{total}, collected samples: {kept}")

    if not datasets:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†è¢«æˆåŠŸæ„å»º")
    merged = ConcatDataset(datasets)
    if verbose:
        print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼Œå…± {len(merged)} æ¡æ ·æœ¬ï¼Œæ–‡ä»¶æ•° {len(datasets)} / {total}")
    return merged


# ----------------------
# å”¯ä¸€çš„ steerï¼šS = mean(hit) - mean(nonhit)
# ----------------------
def build_steer_vector_mean_only(merged_dataset: ConcatDataset) -> torch.Tensor:
    hits = []
    nonhits = []
    for i in range(len(merged_dataset)):
        feat, y = merged_dataset[i]   # feat: [hidden_dim], y: {0.,1.}
        if int(y.item()) == 1:
            hits.append(feat)
        else:
            nonhits.append(feat)

    if len(hits) == 0 or len(nonhits) == 0:
        raise RuntimeError("éœ€è¦åŒæ—¶åŒ…å« å‘½ä¸­(1) ä¸ æœªå‘½ä¸­(0) çš„æ ·æœ¬æ‰èƒ½è®¡ç®—å·®å‘é‡ã€‚")

    mu_hit = torch.stack(hits, dim=0).mean(dim=0)
    mu_non = torch.stack(nonhits, dim=0).mean(dim=0)
    S = mu_hit - mu_non  # ä¸åšä»»ä½•å½’ä¸€åŒ–
    return S, len(hits), len(nonhits)


# ----------------------
# Main (CLI)
# ----------------------
def main():
    parser = argparse.ArgumentParser(
        description="Build steer vector (mixed): mean(H_hit) - mean(H_nonhit), hit = (lexicon OR conf<threshold)"
    )
    parser.add_argument("--layer_id", type=int, required=True)
    parser.add_argument("--jsonl_path", type=str, required=True)
    parser.add_argument("--hidden_dir", type=str, required=True)
    parser.add_argument("--save_path", type=str, required=True)
    parser.add_argument("--threshold", type=float, default=0.7,
                        help="ä½ä¿¡å¿ƒé˜ˆå€¼ï¼šconf < threshold è§†ä¸ºä½ä¿¡å¿ƒ")
    parser.add_argument("--max_files", type=int, default=100)
    parser.add_argument("--expected_offset", type=int, default=1,
                        help="æœŸæœ› V = len(sentence_confidences) - expected_offset")
    parser.add_argument("--verbose", action="store_true")
    args = parser.parse_args()

    merged = batch_build_all_mixed(
        layer_id=args.layer_id,
        jsonl_path=args.jsonl_path,
        hidden_dir=args.hidden_dir,
        threshold=args.threshold,
        max_files=args.max_files,
        expected_offset=args.expected_offset,
        verbose=True
    )

    S, n_hit, n_non = build_steer_vector_mean_only(merged)
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)
    torch.save(S.cpu(), args.save_path)

    l2 = float(torch.linalg.norm(S))
    print(f"âœ… å·²ä¿å­˜ steer å‘é‡åˆ° {args.save_path}")
    print(f"   dim={S.numel()} | ||S||_2 = {l2:.6f} | hit={n_hit} | nonhit={n_non}")

if __name__ == "__main__":
    main()
