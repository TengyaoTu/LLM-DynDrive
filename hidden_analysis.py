# -*- coding: utf-8 -*-
import os
import re
import json
import argparse
from typing import List, Optional

import torch
from torch.utils.data import TensorDataset, ConcatDataset

# =========================
# è¯è¡¨ï¼ˆå‘½ä¸­å³è§†ä¸ºâ€œåŒ…å«è¯æ±‡â€ï¼‰
# =========================
# =========================
# è¯è¡¨ï¼ˆåŸºç¡€è¯å¹²/çŸ­è¯­ï¼‰
# =========================
LEXICON_BASE = [
    # ä¸ç¡®å®š/åæ€ç±»çš„ä½ä¿¡å¿ƒè¯æ±‡
    "alternatively", "alternative", "another", "perhaps", "maybe", "wait", "but",
    "think again", "make sure", "just to ensure", "there any other", "some other",
    "should consider", "about whether", "if they have", "i was", "any errors",
    "or something", "let me check", "hold on", "double check", "however",
    "confusing", "differently", "careful", "sometimes", "alternate"
]

def _normalize_text(s: str) -> str:
    # å¼•å· & ç ´æŠ˜å·/è¿å­—ç¬¦ç»Ÿä¸€
    return (s or "")\
        .replace("â€™", "'").replace("â€˜", "'").replace("â€œ", '"').replace("â€", '"')\
        .replace("â€“", "-").replace("â€”", "-").replace("-", "-")  # en/em/nb-hyphen

def _token_pattern(tok: str) -> str:
    """
    æŠŠä¸€ä¸ªè¯/çŸ­è¯­è½¬æˆæ›´é²æ£’çš„ regex ç‰‡æ®µï¼š
    - ç©ºç™½æŠ˜å ä¸º \s+
    - å…è®¸ 'double[-\s]check' è¿™ç§ç”¨ç©ºæ ¼æˆ–è¿å­—ç¬¦è¿æ¥
    - ç»™å¸¸è§è¯å°¾åŠ å¯é€‰å˜ä½“
    """
    import re

    tok = _normalize_text(tok).lower().strip()
    # å…ˆåˆ†è¯ï¼ˆæŒ‰ç©ºç™½ï¼‰
    parts = re.split(r"\s+", tok)

    def word2regex(w: str) -> str:
        # ä¸ºå…³é”®è¯è¡¥å……å¸¸è§æ´¾ç”Ÿï¼ˆæŒ‰éœ€æ‰©å±•ï¼‰
        if w == "verify":
            return r"verif(?:y|ies|ied|ying|ication(?:s)?)"
        if w == "alternative":
            return r"alternative(?:s|ly)?"
        if w == "another":
            return r"another"
        if w == "perhaps":
            return r"perhaps"
        if w == "maybe":
            return r"maybe"
        if w == "wait":
            return r"wait"
        if w == "but":
            return r"but"
        if w == "however":
            return r"however"
        if w == "confusing":
            return r"confus(?:e|es|ed|ing)"
        if w == "differently":
            return r"different(?:ly)?"
        if w == "careful":
            return r"careful(?:ly)?"
        if w == "sometimes":
            return r"sometimes?"
        if w == "alternate":
            return r"alternat(?:e|es|ed|ing)"
        # é»˜è®¤ä¸¥æ ¼è¯å½¢
        return re.escape(w)

    # å¤„ç†åŒè¯/å¤šè¯çŸ­è¯­çš„è¿æ¥ç¬¦ï¼šå…è®¸ç©ºæ ¼æˆ–è¿å­—ç¬¦
    # ä¾‹å¦‚ "double check" -> r"double(?:[-\s]+)check"
    if len(parts) >= 2:
        pieces = [word2regex(p) for p in parts]
        between = r"(?:[-\s]+)"  # ç©ºæ ¼æˆ–è¿å­—ç¬¦
        body = between.join(pieces)
    else:
        body = word2regex(parts[0])

    # ä¸¤ä¾§è¯è¾¹ç•Œ
    return rf"\b{body}\b"

def _compile_lexicon_regex(lexicon_base):
    import re
    # ä¸“é—¨ä¸ºâ€œdouble check / double-checkâ€è¿™ç§å†åšä¸€ä¸ªçŸ­è¯­åŒä¹‰å½’ä¸€
    expanded = []
    for term in lexicon_base:
        t = term.strip()
        if not t:
            continue  # âœ… åˆ é™¤ç©ºå­—ç¬¦ä¸²å¸¦æ¥çš„å…¨å±€å‘½ä¸­
        # ç‰¹ä¾‹ï¼šåŒè¯ & ä¸‰è¯ä¸­å‡ºç° known è¿æ¥è¯æ—¶è‡ªåŠ¨æ‰©å±•
        if t in ["double check", "make sure", "hold on", "think again",
                 "just to ensure", "there any other", "some other",
                 "should consider", "about whether", "or something",
                 "let me check", "if they have", "i was", "any errors"]:
            expanded.append(_token_pattern(t))
        else:
            expanded.append(_token_pattern(t))

    pattern = "|".join(expanded)
    return re.compile(pattern, flags=re.IGNORECASE)

LEXICON_RE = _compile_lexicon_regex(LEXICON_BASE)

def has_lexicon_hit(text: str) -> int:
    return 1 if LEXICON_RE.search(_normalize_text(text).lower()) else 0
# ----------------------
# Utilities
# ----------------------
def read_jsonl(path: str):
    with open(path, 'r', encoding='utf-8') as f:
        return [json.loads(line.strip()) for line in f if line.strip()]

def split_segments_for_conf(resp_text: str):
    """
    ä»…ç»Ÿè®¡ </think> ä¹‹å‰çš„å†…å®¹ï¼›å†æŒ‰è¿ç»­ç©ºè¡Œåˆ†æ®µã€‚
    """
    text = resp_text or ""
    if "</think>" in text:
        text = text.split("</think>")[0]
    segs = re.split(r"\n\n+", text)
    segs = [s.strip() for s in segs if len(s.strip()) > 0]
    return segs


# ----------------------
# Dataset builder: (feature, label_lex)
# ----------------------
def build_dataset_from_layer(
    layer_id: int,
    json_item: dict,
    hidden_path: str,
    expected_offset: int = 1,
    verbose: bool = False
) -> Optional[TensorDataset]:
    """
    è¿”å›æ¯æ¡æ ·æœ¬äºŒå…ƒç»„: (feature, label_lex)
    çº¦å®šï¼š
    - V = hidden['step'].shape[0]
    - C_raw = len(sentence_confidences)
    - é»˜è®¤æ£€æŸ¥ï¼š V == C_raw - expected_offset
    """
    confs_raw = json_item.get("sentence_confidences", [])
    if not confs_raw or len(confs_raw) <= expected_offset:
        if verbose:
            print("â›” è·³è¿‡ï¼šæ ‡ç­¾ä¸ºç©ºæˆ–è¿‡çŸ­")
        return None

    if not os.path.exists(hidden_path):
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {hidden_path}")
        return None

    try:
        data = torch.load(hidden_path, map_location='cpu', weights_only=False)
    except Exception as e:
        if verbose:
            print(f"âŒ åŠ è½½å¤±è´¥ {hidden_path}: {e}")
        return None

    if layer_id not in data:
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šlayer {layer_id} ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­")
        return None

    layer_data = data[layer_id]
    sample_id = 0
    if sample_id not in layer_data or 'step' not in layer_data[sample_id]:
        if verbose:
            print(f"âŒ è·³è¿‡ï¼šç¼ºå¤± sample_id={sample_id} æˆ– step")
        return None

    tensor = layer_data[sample_id]['step']  # [V, hidden_dim]
    V = tensor.shape[0]
    C_raw = len(confs_raw)

    if V != C_raw - expected_offset:
        if verbose:
            print(f"âŒ å‘é‡æ•° {V} ä¸åŸå§‹ä¿¡å¿ƒæ•° {C_raw} ä¸æ»¡è¶³å·® {expected_offset} -> è·³è¿‡")
        return None

    # è¯è¡¨æ ‡ç­¾ï¼šä»åˆ†æ®µæ–‡æœ¬ç”Ÿæˆï¼Œå¹¶ä¸ expected_offset å¯¹é½
    responses_text = (json_item.get("generated_responses") or [""])[0]
    segs = split_segments_for_conf(responses_text)
    if expected_offset > 0 and len(segs) >= expected_offset:
        segs = segs[expected_offset:]
    lex_labels = [float(has_lexicon_hit(s)) for s in segs]  # 0/1

    # å¯¹é½é•¿åº¦
    m = min(V, len(lex_labels))
    if m <= 0:
        if verbose:
            print("âŒ å¯¹é½åé•¿åº¦ä¸º 0 -> è·³è¿‡")
        return None

    features = tensor[:m].to(dtype=torch.float32)
    labels_lex = torch.tensor(lex_labels[:m], dtype=torch.float32)
    return TensorDataset(features, labels_lex)

def batch_build_all(
    layer_id: int,
    jsonl_path: str,
    hidden_dir: str,
    max_files: int = 100,
    expected_offset: int = 1,
    verbose: bool = True
) -> ConcatDataset:
    data_json = read_jsonl(jsonl_path)
    datasets = []
    total = min(max_files, len(data_json))
    kept = 0
    for i in range(total):
        hidden_path = os.path.join(hidden_dir, f"hidden_{i}.pt")
        ds = build_dataset_from_layer(layer_id, data_json[i], hidden_path,
                                      expected_offset=expected_offset, verbose=False)
        if ds is not None:
            datasets.append(ds)
            kept += len(ds)
        else:
            if verbose:
                print(f"[skip] index={i}")
        if (i + 1) % 50 == 0 and verbose:
            print(f"[progress] processed {i+1}/{total}, collected samples: {kept}")

    if not datasets:
        raise RuntimeError("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†è¢«æˆåŠŸæ„å»º")
    merged = ConcatDataset(datasets)
    if verbose:
        print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼Œå…± {len(merged)} æ¡æ ·æœ¬ï¼Œæ–‡ä»¶æ•° {len(datasets)} / {total}")
    return merged

# ----------------------
# å”¯ä¸€çš„ steerï¼šS = mean(hit) - mean(nonhit)
# ----------------------
def build_steer_vector_mean_only(merged_dataset: ConcatDataset) -> torch.Tensor:
    hits = []
    nonhits = []
    for i in range(len(merged_dataset)):
        feat, y = merged_dataset[i]   # feat: [hidden_dim], y: {0.,1.}
        if int(y.item()) == 1:
            hits.append(feat)
        else:
            nonhits.append(feat)

    if len(hits) == 0 or len(nonhits) == 0:
        raise RuntimeError("éœ€è¦åŒæ—¶åŒ…å« å‘½ä¸­(1) ä¸ æœªå‘½ä¸­(0) çš„æ ·æœ¬æ‰èƒ½è®¡ç®—å·®å‘é‡ã€‚")

    mu_hit = torch.stack(hits, dim=0).mean(dim=0)
    mu_non = torch.stack(nonhits, dim=0).mean(dim=0)
    S = mu_hit - mu_non  # ä¸åšä»»ä½•å½’ä¸€åŒ–
    return S

# ----------------------
# Main (CLI)
# ----------------------
def main():
    parser = argparse.ArgumentParser(description="Build steer vector = mean(hit) - mean(nonhit) from hidden states")
    parser.add_argument("--layer_id", type=int, required=True)
    parser.add_argument("--jsonl_path", type=str, required=True)
    parser.add_argument("--hidden_dir", type=str, required=True)
    parser.add_argument("--save_path", type=str, required=True)
    parser.add_argument("--max_files", type=int, default=100)
    parser.add_argument("--expected_offset", type=int, default=1, help="æœŸæœ› V = C_raw - expected_offset")
    parser.add_argument("--verbose", action="store_true")
    args = parser.parse_args()

    merged = batch_build_all(
        layer_id=args.layer_id,
        jsonl_path=args.jsonl_path,
        hidden_dir=args.hidden_dir,
        max_files=args.max_files,
        expected_offset=args.expected_offset,
        verbose=True
    )

    S = build_steer_vector_mean_only(merged)
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)
    torch.save(S.cpu(), args.save_path)

    print(f"âœ… å·²ä¿å­˜ steer å‘é‡åˆ° {args.save_path}")
    print(f"   dim={S.numel()} | ||S||_2 = {float(torch.linalg.norm(S)):.6f}")

if __name__ == "__main__":
    main()
